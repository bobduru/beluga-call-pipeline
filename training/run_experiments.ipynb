{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel/Documents/belugas/beluga-call-pipeline\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Walk up until we find the project root (folder with the .env)\n",
    "current_path = Path().resolve()\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if (parent / \".env\").exists():\n",
    "        load_dotenv(parent / \".env\")\n",
    "        project_root = os.getenv(\"PROJECT_ROOT\")\n",
    "        print(project_root)\n",
    "        sys.path.append(project_root)     \n",
    "        break\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from models.resnet import ResnetMultilabel\n",
    "from models.mobilenet import MobileNetMultilabel\n",
    "from models.quant_mobilenet import load_mobilenet_v3_quant\n",
    "\n",
    "from training.cross_validation import run_cross_val, train_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Optimization Experiments\n",
    "\n",
    "This section covers the model optimization experiments:\n",
    "- Switching from **ResNet18** to **MobileNet V3 Small**,\n",
    "- Further**Truncating** the MobileNet architecture,\n",
    "- Applying **8-bit Quantization-Aware Training (QAT)** on the MobileNet model.\n",
    "\n",
    "Each experiment is executed with **cross-validation** as described in the paper.  \n",
    "Results are written to a dedicated `results/` directory and subsequently examined in `analysis.ipynb`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"../data/labels/Overlaps_1s.csv\")\n",
    "labels_df[\"ClipFilenamePt\"] = labels_df[\"ClipFilename\"] + \".pt\"\n",
    "\n",
    "\n",
    "label_columns = [\"ECHO\", \"HFPC\", \"CC\", \"Whistle\"]\n",
    "\n",
    "data_dir = \"../data\"\n",
    "processed_spects_dir = data_dir + \"/Full_Dataset/Overlaps_1s_hp_1024_resize/\"\n",
    "\n",
    "results_dir = \"./results/model_optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config_default = {\n",
    "    \"batch_size\": 32,\n",
    "    \"lr_decay_factor\": 0.5,\n",
    "    \"patience_lr\": 2,\n",
    "    \"n_epochs\": 2, #100\n",
    "    \"min_epochs\": 1, #15\n",
    "    \"patience_early_stopping\": 5,\n",
    "    \"metric_mode\": \"max\",\n",
    "    \"val_metric\": \"f1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resnet18 model on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuel/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/emmanuel/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for fold 1 of 5\n",
      "\n",
      "Fold 1 dataset sizes:\n",
      "Train: 6996 samples\n",
      "Val: 1750 samples\n",
      "Test: 2187 samples\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Model must be generated with load_mobilenet_v3_quant() when use_quantization=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_cross_val\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mResnetMultilabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_spects_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config_default\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/training/cross_validation.py:374\u001b[0m, in \u001b[0;36mrun_cross_val\u001b[0;34m(labels_df, label_columns, model_class, processed_spects_dir, run_name, model_kwargs, n_splits, training_config, save_models, use_quantization, results_dir, compute_sites_metrics)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 374\u001b[0m     base_dir, test_prediction_df,training_details \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_spects_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_spects_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_sites_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_sites_metrics\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     test_predictions_dfs\u001b[38;5;241m.\u001b[39mappend(test_prediction_df)\n\u001b[1;32m    393\u001b[0m test_predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(test_predictions_dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/training/cross_validation.py:140\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(labels_df, label_columns, model, train_data, val_data, test_data, run_name, training_config, processed_spects_dir, results_dir, fold_idx, use_quantization, save_models, base_dir, compute_sites_metrics)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Verify model type if quantization is enabled\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_quantization \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, QuantizableMobileNetV3):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel must be generated with load_mobilenet_v3_quant() when use_quantization=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    146\u001b[0m device \u001b[38;5;241m=\u001b[39m get_best_device()\n\u001b[1;32m    147\u001b[0m cpu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Model must be generated with load_mobilenet_v3_quant() when use_quantization=True"
     ]
    }
   ],
   "source": [
    "run_cross_val(\n",
    "    labels_df, \n",
    "    label_columns, \n",
    "    ResnetMultilabel,  \n",
    "    processed_spects_dir,\n",
    "    run_name=\"resnet\",\n",
    "    model_kwargs={\n",
    "        \"pretrained\":True,\n",
    "    }, \n",
    "    training_config=training_config_default,\n",
    "    save_models=True,\n",
    "    use_quantization=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running all MobileNet variants: layer depth & quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_2_layers\n",
      "n_layers: 2, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_2_layers\n",
      "n_layers: 2, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_4_layers\n",
      "n_layers: 4, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_4_layers\n",
      "n_layers: 4, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_6_layers\n",
      "n_layers: 6, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_6_layers\n",
      "n_layers: 6, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_8_layers\n",
      "n_layers: 8, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_8_layers\n",
      "n_layers: 8, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_10_layers\n",
      "n_layers: 10, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_10_layers\n",
      "n_layers: 10, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_12_layers\n",
      "n_layers: 12, quantization: False\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Running experiment: mobile_net_qat_12_layers\n",
      "n_layers: 12, quantization: True\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "All experiments completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_layers_to_test = [2, 4, 6, 8, 10, 12,]  \n",
    "quantization_options = [False, True]\n",
    "\n",
    "for n_layers in n_layers_to_test:\n",
    "    for use_quantization in quantization_options:\n",
    "        # Create run name based on parameters\n",
    "        quant_suffix = \"_qat\" if use_quantization else \"\"\n",
    "        run_name = f\"mobile_net{quant_suffix}_{n_layers}_layers\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Running experiment: {run_name}\")\n",
    "        print(f\"n_layers: {n_layers}, quantization: {use_quantization}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        model_class = load_mobilenet_v3_quant if use_quantization else MobileNetMultilabel\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"pretrained\": True,\n",
    "            \"n_layers\": n_layers\n",
    "        }\n",
    "\n",
    "        if use_quantization:\n",
    "            model_kwargs[\"qat\"] = True\n",
    "        \n",
    "        try:\n",
    "            run_cross_val(\n",
    "                labels_df, \n",
    "                label_columns, \n",
    "                model_class,  \n",
    "                processed_spects_dir,\n",
    "                run_name=run_name,\n",
    "                model_kwargs=model_kwargs, \n",
    "                n_splits=5,\n",
    "                training_config=training_config_default,\n",
    "                save_models=True,\n",
    "                use_quantization=use_quantization,\n",
    "            )\n",
    "            print(f\"✅ Successfully completed: {run_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in experiment {run_name}: {str(e)}\")\n",
    "            print(f\"Continuing with next experiment...\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Function call template to run quick experiments<h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_val(\n",
    "    labels_df, \n",
    "    label_columns, \n",
    "    MobileNetMultilabel,  \n",
    "    processed_spects_dir,\n",
    "    run_name=\"mobile_net_hp_1024_8_layers_all_absences\",\n",
    "    model_kwargs={\n",
    "        \"pretrained\":True,\n",
    "        \"n_layers\": 8\n",
    "    }, \n",
    "    n_splits=5,\n",
    "    training_config=training_config_default,\n",
    "    save_models=True,\n",
    "    use_quantization=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site Generalization Experiments\n",
    "\n",
    "\n",
    "1. **Site-specific models** — train a separate model per site.\n",
    "2. **Leave-One-Site-Out** — train on all but one site and test on the held-out site to assess generalizability.\n",
    "\n",
    "All runs follow the protocol described in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.cross_validation import create_test_fold_indices\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from models.utils import aggregate_folds_testing_metrics\n",
    "\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"../data/labels/Overlaps_1s.csv\")\n",
    "labels_df[\"ClipFilenamePt\"] = labels_df[\"ClipFilename\"] + \".pt\"\n",
    "\n",
    "\n",
    "label_columns = [\"ECHO\", \"HFPC\", \"CC\", \"Whistle\"]\n",
    "\n",
    "data_dir = \"../data\"\n",
    "processed_spects_dir = data_dir + \"/Full_Dataset/Overlaps_1s_hp_1024_resize/\"\n",
    "\n",
    "\n",
    "labels_df = create_test_fold_indices(labels_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site-specific models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileNetV3_Small model on mps\n",
      "Number of feature layers: 13\n",
      "Using up to layer 8\n",
      "Feature size after backbone: 48\n",
      "\n",
      "Training Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:02<00:01,  5.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_quantization:\n\u001b[1;32m     25\u001b[0m         run_name \u001b[38;5;241m=\u001b[39m run_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_qat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     run_dir, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_spects_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_spects_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/sites_generalization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config_default\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_quantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_sites_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m aggregate_folds_testing_metrics(run_dir)\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/training/cross_validation.py:182\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(labels_df, label_columns, model, train_data, val_data, test_data, run_name, training_config, processed_spects_dir, results_dir, fold_idx, use_quantization, save_models, base_dir, compute_sites_metrics)\u001b[0m\n\u001b[1;32m    178\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultilabelTrainer(model, device\u001b[38;5;241m=\u001b[39mdevice, labels_mapping\u001b[38;5;241m=\u001b[39mlabel_columns)\n\u001b[1;32m    180\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m setup_multilabel_dataloaders(train_data, val_data, test_data, processed_spects_dir, label_columns)\n\u001b[0;32m--> 182\u001b[0m trained_model, training_details, test_predictions_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience_early_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatience_early_stopping\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_metric\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_metric_goal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_test_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_quantization\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_models:\n\u001b[1;32m    201\u001b[0m     save_model(trained_model,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/models/trainer.py:58\u001b[0m, in \u001b[0;36mMultilabelTrainer.fit\u001b[0;34m(self, train_loader, val_loader, test_loader, loss_fn, optimizer, scheduler, n_epochs, patience_early_stopping, metrics, labels_mapping, val_metric, val_metric_goal, val_interval, final_training, keep_test_logs, qat, min_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     57\u001b[0m     val_metric_res \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m val_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m n_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     63\u001b[0m         val_results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_epoch(\n\u001b[1;32m     64\u001b[0m             epoch, val_loader, loss_fn, metrics, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/models/trainer.py:143\u001b[0m, in \u001b[0;36mMultilabelTrainer.train_epoch\u001b[0;34m(self, epoch, train_loader, loss_fn, optimizer, metrics)\u001b[0m\n\u001b[1;32m    140\u001b[0m iterator \u001b[38;5;241m=\u001b[39m tqdm(train_loader)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (features, labels, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterator):\n\u001b[0;32m--> 143\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_train_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     epoch_loss_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    148\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/models/trainer.py:179\u001b[0m, in \u001b[0;36mMultilabelTrainer.process_train_batch\u001b[0;34m(self, batch_idx, features, labels, loss_fn, optimizer, metrics)\u001b[0m\n\u001b[1;32m    176\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    177\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/models/trainer.py:247\u001b[0m, in \u001b[0;36mMultilabelTrainer.update_metrics\u001b[0;34m(self, metrics, labels, outputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, metrics, labels, outputs):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 247\u001b[0m         \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torchmetrics/metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torchmetrics/classification/stat_scores.py:489\u001b[0m, in \u001b[0;36mMultilabelStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 489\u001b[0m     \u001b[43m_multilabel_stat_scores_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_format(\n\u001b[1;32m    493\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    495\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_update(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average)\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:627\u001b[0m, in \u001b[0;36m_multilabel_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_labels, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected both `target.shape[1]` and `preds.shape[1]` to be equal to the number of labels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m     )\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Check that target only contains [0,1] values or value in ignore_index\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     check \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39many((unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torch/_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torch/functional.py:997\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 997\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/belugas/beluga-call-pipeline/.venv/lib/python3.12/site-packages/torch/functional.py:911\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    903\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    905\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 911\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_sites = [\"RDL\", \"CAC\", \"BSM\", \"KAM\" ]\n",
    "\n",
    "use_quantization = False\n",
    "\n",
    "for train_site in all_sites:\n",
    "\n",
    "    for fold_idx in range(5):\n",
    "        \n",
    "        model_class = load_mobilenet_v3_quant if use_quantization else MobileNetMultilabel\n",
    "\n",
    "        model = model_class(\n",
    "            pretrained=True,\n",
    "            n_layers=8,\n",
    "            num_classes=len(label_columns)\n",
    "        )\n",
    "\n",
    "        train_site_df = labels_df[labels_df[\"Site\"]==train_site]\n",
    "        train_data = train_site_df[train_site_df[\"test_fold_idx\"] != fold_idx]\n",
    "        train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['Site'])\n",
    "        \n",
    "        test_data = train_site_df[train_site_df[\"test_fold_idx\"] == fold_idx]\n",
    "\n",
    "        run_name = f\"{train_site}_only\"\n",
    "        if use_quantization:\n",
    "            run_name = run_name + \"_qat\"\n",
    "\n",
    "\n",
    "        run_dir, _, _ = train_model(\n",
    "            labels_df,\n",
    "            label_columns,\n",
    "            model,\n",
    "            train_data,\n",
    "            val_data,\n",
    "            test_data,\n",
    "            processed_spects_dir=processed_spects_dir,\n",
    "            fold_idx=fold_idx,\n",
    "            results_dir=\"./results/sites_generalization\",\n",
    "            run_name=run_name,\n",
    "            training_config=training_config_default,\n",
    "            use_quantization=use_quantization,\n",
    "            compute_sites_metrics=True\n",
    "        )\n",
    "\n",
    "    aggregate_folds_testing_metrics(run_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Site out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_sites = [\"BSM\", \"RDL\", \"CAC\", \"KAM\" ]\n",
    "\n",
    "use_quantization = False\n",
    "\n",
    "for out_site in all_sites:\n",
    "    for fold_idx in range(5):\n",
    "        \n",
    "        model_class = load_mobilenet_v3_quant if use_quantization else MobileNetMultilabel\n",
    "\n",
    "        model = model_class(\n",
    "            pretrained=True,\n",
    "            n_layers=8,\n",
    "            num_classes=len(label_columns)\n",
    "        )\n",
    "\n",
    "        train_sites = [site for site in all_sites if site != out_site]\n",
    "\n",
    "        train_df = labels_df[labels_df[\"test_fold_idx\"] != fold_idx]\n",
    "        \n",
    "        train_sites_df = train_df[train_df[\"Site\"].isin(train_sites)]\n",
    "\n",
    "        train_data, val_data = train_test_split(train_sites_df, test_size=0.2, random_state=42, stratify=train_sites_df['Site'])\n",
    "        \n",
    "        test_data = labels_df[labels_df[\"test_fold_idx\"] == fold_idx]\n",
    "\n",
    "\n",
    "        run_name = f\"leave_{out_site}_out\"\n",
    "        if use_quantization:\n",
    "            run_name = run_name + \"_qat\"\n",
    "\n",
    "        print(run_name)\n",
    "        print(f\"Site out : {out_site}\")\n",
    "        print(\"Train df\")\n",
    "        print(train_data[\"Site\"].value_counts())\n",
    "        print(\"\\nVal df\")\n",
    "        print(val_data[\"Site\"].value_counts())\n",
    "\n",
    "        # break\n",
    "        run_dir, _, _ = train_model(\n",
    "            labels_df,\n",
    "            label_columns,\n",
    "            model,\n",
    "            train_data,\n",
    "            val_data,\n",
    "            test_data,\n",
    "            processed_spects_dir=processed_spects_dir,\n",
    "            fold_idx=fold_idx,\n",
    "            run_name=run_name,\n",
    "            results_dir=\"./results/sites_generalization\",\n",
    "            training_config=training_config_default,\n",
    "            use_quantization=use_quantization,\n",
    "            \n",
    "        )\n",
    "    \n",
    "    aggregate_folds_testing_metrics(run_dir)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training the final model on all the data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.cross_validation_2 import create_test_fold_indices\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from models.utils import aggregate_folds_testing_metrics\n",
    "\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv(\"../data/labels/Overlaps_1s.csv\")\n",
    "labels_df[\"ClipFilenamePt\"] = labels_df[\"ClipFilename\"] + \".pt\"\n",
    "\n",
    "\n",
    "label_columns = [\"ECHO\", \"HFPC\", \"CC\", \"Whistle\"]\n",
    "\n",
    "data_dir = \"../data\"\n",
    "processed_spects_dir = data_dir + \"/Full_Dataset/Overlaps_1s_hp_1024_resize/\"\n",
    "\n",
    "results_dir = \"./final_results\"\n",
    "\n",
    "labels_df = create_test_fold_indices(labels_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_quantization = False\n",
    "        \n",
    "model_class = load_mobilenet_v3_quant if use_quantization else MobileNetMultilabel\n",
    "\n",
    "model = model_class(\n",
    "    pretrained=True,\n",
    "    n_layers=8,\n",
    "    num_classes=len(label_columns)\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_data['Site'])\n",
    "test_data = val_data #Doesn't matter here, won't be used anyway\n",
    "\n",
    "run_name = f\"Final_model\"\n",
    "if use_quantization:\n",
    "    run_name = run_name + \"_qat\"\n",
    "\n",
    "run_dir = train_model(\n",
    "    labels_df,\n",
    "    label_columns,\n",
    "    model,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    test_data,\n",
    "    fold_idx=0,\n",
    "    processed_spects_dir=processed_spects_dir,\n",
    "    run_name=run_name,\n",
    "    results_dir=\"results/final_model\",\n",
    "    training_config=training_config_default,\n",
    "    use_quantization=use_quantization,\n",
    "    save_model=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
